{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f13299-76d1-47bb-9413-2ef3d6e73bce",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae437d-72a4-408d-baf5-cb79894fa6b5",
   "metadata": {},
   "source": [
    "## Pre-processing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff82d067-bbc5-4ef0-8598-31573b6ca467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pandas as pd\n",
    "import stanza\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from markdown import markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5f4b0-9374-4a9c-86de-e46f44eae2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = STOPWORDS.union((\n",
    "    \"var\", \"variable\", \"computed\", \"costa\", \"botocore\", \"version\", \"step\",\n",
    "    \"support\", \"source\", \"hashicorp\", \"service\", \"branch\", \"pull\", \"merge\", \"issue\",\n",
    "    \"pr\", \"galoy-pay\", \"bumped\", \"add\", \"payload\", \"boto\", \"accurics\", \"hana\",\n",
    "    \"bump\", \"added\", \"latest\", \"update\", \"tf\", \"github\", \"test\", \"sourced\",\n",
    "    \"instead\", \"use\", \"plan\", \"updates\", \"diff\", \"bump-galoy-pay-image\", \"draft\",\n",
    "    \"iam\", \"i'm\", \"v1\", \"apply\", \"fix\", \"fixes\", \"kvo\", \"needed\", \"tco\", \"create\",\n",
    "    \"run\", \"code\", \"feat\", \"lambda\", \"need\", \"link\", \"project\", \"new\", \"change\",\n",
    "    \"they're\"\n",
    "))\n",
    "\n",
    "UPOS = ('PROPN', 'NOUN', 'VERB', 'ADJ', 'ADV')\n",
    "nlp_pipeline = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "def load_dataset(filepath):\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def clean_markup(doc):\n",
    "    # Convert Markdown to HTML markup\n",
    "    clean_doc = markdown(doc,extensions=['fenced_code'])\n",
    "    clean_doc = BeautifulSoup(clean_doc)\n",
    "    # Remove unwanted content\n",
    "    for s in clean_doc.select('code'):\n",
    "        s.extract()\n",
    "    for s in clean_doc.select('pre'):\n",
    "        s.extract()\n",
    "    for s in clean_doc.select('blockquotes'):\n",
    "        s.extract()\n",
    "    # Remove HTML markup\n",
    "    clean_doc = ''.join(clean_doc.findAll(text=True))\n",
    "    # Remove URLs\n",
    "    clean_doc = re.sub(r'\\S*https?:\\S*', '', clean_doc, flags=re.MULTILINE)\n",
    "    \n",
    "    return clean_doc\n",
    "    \n",
    "def prepare_document(doc):\n",
    "    clean_doc = clean_markup(doc)\n",
    "\n",
    "    tokens = [token.to_dict()[0][\"lemma\"]\n",
    "                for token in nlp_pipeline(clean_doc).iter_tokens()\n",
    "                if token.to_dict()[0][\"upos\"] in UPOS and not token.to_dict()[0][\"text\"] in STOPWORDS\n",
    "            ]\n",
    "    return tokens\n",
    "\n",
    "def prepare_corpus(documents):\n",
    "    corpus = []\n",
    "    total_docs = len(documents)\n",
    "\n",
    "    for i in range(total_docs):\n",
    "        print(f'{total_docs} documents: {(i+1)/total_docs*100:.2f}% parsed')\n",
    "        print(documents[i])\n",
    "        corpus.append(prepare_document(documents[i]))\n",
    "        clear_output(wait=False)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "def build_tfidf_model(corpus):\n",
    "    corpus_dict = Dictionary(corpus)\n",
    "    corpus_bow = tuple(corpus_dict.doc2bow(sentence) for sentence in corpus)\n",
    "    tfidf_model = TfidfModel(corpus_bow, normalize=True)\n",
    "\n",
    "    return corpus_dict, corpus_bow, tfidf_model\n",
    "\n",
    "def get_keywords(model, num_topics=-1, num_words=5):\n",
    "    topic_vectors = model.show_topics(num_topics=num_topics, num_words=num_words, formatted=False)\n",
    "    return sorted(tuple(set([w[0] for t in topic_vectors for w in t[1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b64247-25b0-4544-acaf-65520bf3511d",
   "metadata": {},
   "source": [
    "## Commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b0de57d-2b5a-431e-a692-b363784daf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('data/dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da584d87-6b62-4541-852c-7683aa6e9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [unit['content']['message'] for unit in data if unit['type'] == 'commit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b68661-1100-4713-b4fe-dfc04bf41590",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = prepare_corpus(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad5e535-712c-48a9-900f-769e70ab3b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "(corpus_dict, corpus_bow, tfidf_model) = build_tfidf_model(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186df97d-ae8c-409e-95a1-dd26173cfe05",
   "metadata": {},
   "source": [
    "Explore hyperparameters\n",
    "- K = {5,6,...,34,35}\n",
    "- alpha = {0.01,50/K}\n",
    "- beta = {0.01,50/K}\n",
    "- chunksize = {1,2,4,8,...,1024}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670cac5c-2ba6-451d-bb8d-04693469b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_topics in range(5,36):\n",
    "    for alpha in (0.01, 50/num_topics):\n",
    "        for beta in (0.01, 50/num_topics):\n",
    "            for chunksize in (1,2,4,8,16,32,64,128,256,512,1024):\n",
    "                lda_model = LdaModel(corpus=corpus_bow,id2word=corpus_dict,num_topics=num_topics,alpha=alpha,eta=beta,chunksize=chunksize,passes=100)\n",
    "                perplexity = lda_model.log_perplexity(corpus_bow)\n",
    "                coherence_model_lda = CoherenceModel(model=lda_model, texts=corpus, dictionary=corpus_dict, coherence='c_v')\n",
    "                coherence_lda = coherence_model_lda.get_coherence()\n",
    "                print(f'{num_topics: <{2}} {alpha:.{2}} {beta:.{2}} {chunksize: <{4}} | {perplexity: >+{5}.{2}f} {coherence_lda: >{4}.{2}f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e21623-032d-4ed3-96b1-d98dc5d7d76b",
   "metadata": {},
   "source": [
    "Train models for manual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4dcc8-f7a2-4924-9819-d1c7c1077fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {'num_topics':5, 'alpha':50/5, 'eta':0.01, 'chunksize':10},\n",
    "    {'num_topics':9, 'alpha':50/9, 'eta':0.01, 'chunksize':16},\n",
    "    {'num_topics':11, 'alpha':50/11, 'eta':0.01, 'chunksize':32},\n",
    "    {'num_topics':12, 'alpha':50/12, 'eta':0.01, 'chunksize':32}, # SELECTED MODEL\n",
    "    {'num_topics':14, 'alpha':50/14, 'eta':0.01, 'chunksize':32},\n",
    "    {'num_topics':13, 'alpha':50/13, 'eta':0.01, 'chunksize':32},\n",
    "    {'num_topics':16, 'alpha':50/16, 'eta':0.01, 'chunksize':32}\n",
    "]\n",
    "\n",
    "lda_models = []\n",
    "\n",
    "for conf in configs:\n",
    "    lda_models.append(LdaModel(corpus=corpus_bow,id2word=corpus_dict,passes=100,**conf))\n",
    "    perplexity = lda_models[-1].log_perplexity(corpus_bow)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_models[-1], texts=corpus, dictionary=corpus_dict, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(f\"{conf['num_topics']: <{2}} {conf['alpha']:.{2}} {conf['eta']:.{2}} {conf['chunksize']: <{4}} | {perplexity: >+{5}.{2}f} {coherence_lda: >{4}.{2}f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d219e70-6004-4f11-b997-b5543b4769a4",
   "metadata": {},
   "source": [
    "Inpect models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d454a8-d146-44a4-b5d6-1d190b346b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick model for inspection\n",
    "model_num = 3 # selected for the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def839cf-ffec-4066-9e1e-d33e8cf1225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_models[model_num], corpus_bow, corpus_dict)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9348adcd-12b2-4c66-a4c3-ce160840f4a2",
   "metadata": {},
   "source": [
    "Print top words in vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3e6c4e-80fb-496b-b80f-ec989f105021",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vectors = lda_model.show_topics(num_topics=configs[model_num]['num_topics'], num_words=10, formatted=False)\n",
    "for v in topic_vectors:\n",
    "    print([w[0] for w in v[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bebf1f7-6053-4205-b76f-e2317862953a",
   "metadata": {},
   "source": [
    "### Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6dc6e5b-58d9-4a1f-bbce-5cc905f0b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('data/dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c58873b-a4da-4f48-a892-71391592cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for unit in data:\n",
    "    if unit['type'] == 'issue':\n",
    "        document = ''\n",
    "        if unit['content']['title']:\n",
    "            document += '\\n'+ unit['content']['title']\n",
    "        if unit['content']['body']:\n",
    "            document += '\\n'+ unit['content']['body']\n",
    "        document += '\\n'.join([c for c in unit['content']['comments']])\n",
    "        documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "172fd0ed-3e81-43d5-b39a-efc7620f815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = prepare_corpus(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c60c6c0f-1eaa-449e-a764-7bac6909417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(corpus_dict, corpus_bow, tfidf_model) = build_tfidf_model(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd650f09-732b-456c-a9e7-c13a8da09158",
   "metadata": {},
   "source": [
    "Explore hyperparameters\n",
    "- K = {5,6,...,34,35}\n",
    "- alpha = {0.01,50/K}\n",
    "- beta = {0.01,50/K}\n",
    "- chunksize = {1,2,4,8,...,1024}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf031e-a06a-4559-a40d-a06d69e86164",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_topics in range(5,36):\n",
    "    for alpha in (0.01, 50/num_topics):\n",
    "        for beta in (0.01, 50/num_topics):\n",
    "            for chunksize in (1,2,4,8,16,32,64,128,256,512,1024):\n",
    "                lda_model = LdaModel(corpus=corpus_bow,id2word=corpus_dict,num_topics=num_topics,alpha=alpha,eta=beta,chunksize=chunksize,passes=100)\n",
    "                perplexity = lda_model.log_perplexity(corpus_bow)\n",
    "                coherence_model_lda = CoherenceModel(model=lda_model, texts=corpus, dictionary=corpus_dict, coherence='c_v')\n",
    "                coherence_lda = coherence_model_lda.get_coherence()\n",
    "                print(f'{num_topics: <{2}} {alpha:.{2}} {beta:.{2}} {chunksize: <{4}} | {perplexity: >+{5}.{2}f} {coherence_lda: >{4}.{2}f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cde954-a923-4029-bfeb-5b9e5af6d656",
   "metadata": {},
   "source": [
    "Train models for manual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddff525-eeb2-485e-9b3e-d1fc73977f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {'num_topics':5, 'alpha':0.01, 'eta':50/5, 'chunksize':2}, # SELECTED MODEL\n",
    "    {'num_topics':12, 'alpha':50/12, 'eta':0.01, 'chunksize':2},\n",
    "    {'num_topics':20, 'alpha':0.01, 'eta':50/20, 'chunksize':256},\n",
    "    {'num_topics':21, 'alpha':50/21, 'eta':0.01, 'chunksize':2},\n",
    "    {'num_topics':27, 'alpha':50/27, 'eta':0.01, 'chunksize':2},\n",
    "    {'num_topics':33, 'alpha':50/33, 'eta':0.01, 'chunksize':4}\n",
    "]\n",
    "\n",
    "lda_models = []\n",
    "\n",
    "for conf in configs:\n",
    "    lda_models.append(LdaModel(corpus=corpus_bow,id2word=corpus_dict,passes=100,**conf))\n",
    "    perplexity = lda_models[-1].log_perplexity(corpus_bow)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_models[-1], texts=corpus, dictionary=corpus_dict, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(f\"{conf['num_topics']: <{2}} {conf['alpha']:.{2}} {conf['eta']:.{2}} {conf['chunksize']: <{4}} | {perplexity: >+{5}.{2}f} {coherence_lda: >{4}.{2}f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5670d7d0-1ad2-4639-8b2b-1dbc9c60d9ee",
   "metadata": {},
   "source": [
    "Inspect models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916eaa9-5d5c-4f16-abf8-f02d5bd772c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick model for inspection\n",
    "model_num = 0 # selected for the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75303478-52a3-4ebd-8a1c-c8aed06e8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_models[model_num], corpus_bow, corpus_dict)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a655153-ddb9-49cb-ba2b-77d7ab692948",
   "metadata": {},
   "source": [
    "Print top words in vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479fdc2-7ff2-48ee-9443-19ef2052b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vectors = lda_model.show_topics(num_topics=configs[model_num]['num_topics'], num_words=10, formatted=False)\n",
    "for v in topic_vectors:\n",
    "    print([w[0] for w in v[1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (main, Dec 23 2022, 09:28:24) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
