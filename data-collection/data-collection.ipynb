{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## GitHub credentials\n",
    "\n",
    "A private access token is necessary to make use of less restrictive API limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from github import RateLimitExceededException, Github\n",
    "\n",
    "# Providing access token\n",
    "access_token = \"< YOUR PRIVATE ACCESS TOKEN >\"\n",
    "g = Github(login_or_token=access_token)\n",
    "\n",
    "# Confirm your login is successful\n",
    "user = g.get_user()\n",
    "print(f\"Authenticated as: {user.login}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data files\n",
    "\n",
    "Path to output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "STEP1_HCLREPOS =   os.path.join(\"data\", \"step1-hcl-repositories.txt\")\n",
    "STEP2_TFREPOS =    os.path.join(\"data\", \"step2-tf-repositories.txt\")\n",
    "STEP2_404REPOS =   os.path.join(\"data\", \"step2-404-repositories.txt\")\n",
    "STEP3_KWCOMMITS =  os.path.join(\"data\", \"step3-keyword-commits.json\")\n",
    "STEP3_ERRORREPOS = os.path.join(\"data\", \"step3-error-repositories.txt\")\n",
    "STEP4_TFCOMMITS =  os.path.join(\"data\", \"step4-tf-commits.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 1 - Recover GitHub repositories containing HCL IaC\n",
    "\n",
    "For each day from 2014, query the GitHub search API for repositories that use HCL as language.\n",
    "Some dates queried do not exist, an exception is caught to avoid interruptions.\n",
    "\n",
    "Every repository is saved in '`data/step1-hcl-repositories.txt`' so no progress is lost in case of interruptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "script_urls = []\n",
    "for year in range(2014, 2023):\n",
    "    for month in range(1, 13):\n",
    "        print(f\"Scraping month {month} of year {year}\")\n",
    "        for day in range(1, 32):\n",
    "            # Formatting compatible with search parameters\n",
    "            date = f\"{year}-{month:02d}-{day:02d}\"\n",
    "            try:\n",
    "                time.sleep(2)  # sleep to reset API search limit\n",
    "                repos = g.search_repositories(query=f\"created:{date} language:HCL\")\n",
    "                for repo in repos:\n",
    "                    time.sleep(0.2)  # sleep to reset API core limit\n",
    "                    # URLs are added to a txt file to avoid data loss\n",
    "                    with open(STEP1_HCLREPOS, \"a\") as file:\n",
    "                        file.write(f\"{repo.clone_url}\\n\")\n",
    "                    script_urls.append(repo.clone_url)\n",
    "            except RateLimitExceededException:\n",
    "                print(\"Rate Limit Exception reached!\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                # These are impossible dates (31-2-2022)\n",
    "                print(f\"Skipping: {date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Number of HCL repositories obtained\n",
    "print(len(script_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 2 - Filter repositories with Terraform files\n",
    "\n",
    "Read the repositories from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%# read urls from the file and strip the '\\n'\n"
    }
   },
   "outputs": [],
   "source": [
    "# read urls from the file and strip the '\\n'\n",
    "gitUrls_file = open(STEP1_HCLREPOS, \"r\")\n",
    "repo_links = gitUrls_file.readlines()\n",
    "repo_links = [repo.strip() for repo in repo_links]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Scan the content of each repository looking for files with extension '`.tf`' and '`.tf.json`' (i.e., Terraform artifact files).\n",
    "\n",
    "Suitable repositories are saved in '`data/step2-tf-repositories.txt`'.\n",
    "\n",
    "Repositories that are not reachable for any reason are saved in '`data/step2-404-repositories.txt`'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "terraform_keywords = ['.tf', '.tf.json']\n",
    "terraform_relevant_repos = []\n",
    "for repo_url in repo_links:\n",
    "    if counter % 100 == 0:\n",
    "        print(f'Got to {counter}')\n",
    "    try:\n",
    "        time.sleep(2)  # sleep for API search limit\n",
    "        split_list = repo_url.split(\"/\")\n",
    "        actual_url = split_list[3]+ '/' + split_list[4]\n",
    "        repo = g.get_repo(actual_url.split('.git')[0])\n",
    "        contents = repo.get_contents('')\n",
    "        while contents:\n",
    "            time.sleep(0.2)  # sleep for API core limit\n",
    "            file_content = contents.pop(0)\n",
    "            if file_content.type == \"dir\":\n",
    "                contents.extend(repo.get_contents(file_content.path))\n",
    "            else:\n",
    "                if file_content.name is not None and any(key in file_content.name.lower() for key in terraform_keywords):\n",
    "                    terraform_relevant_repos.append(repo_url)\n",
    "                    with open(STEP2_TFREPOS, \"a\") as file:\n",
    "                        file.write(f\"{repo_url}\\n\")\n",
    "                    break\n",
    "        counter += 1\n",
    "    except RateLimitExceededException:\n",
    "        print(\"Rate Limit Exception reached!\")\n",
    "    except Exception as e:\n",
    "        print(f\"{e}\\n{repo_url}\")\n",
    "        with open(STEP2_404REPOS, \"a\") as file:\n",
    "            file.write(f\"{repo_url}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 3 - Extract commits with cost-related keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read the previously filtered Terraform repositories. \n",
    "Then style a keyword list meant to be used in the commit message filtering phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read urls from the file and strip the '\\n'\n",
    "all_repos = open(STEP2_TFREPOS, \"r\")\n",
    "repo_links = [repo.strip() for repo in all_repos.readlines()]\n",
    "cost_keywords = [\"cheap\", \"expens\", \"cost\", \"efficient\", \"bill\", \"pay\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using PyDriller, traverse the commits of each repository.\n",
    "For every commit containing one or more keywords in its message, extract **commit id**, **message**, **date** and **list of modified files**.\n",
    "\n",
    "The final list of extracted commits is saved as JSON in '`data/step3-keyword-commits.json`'.\n",
    "\n",
    "If an error occur while trying to access a commit, the repository URL is saved in '`data/step3-error-repositories.txt`'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pydriller import Repository\n",
    "\n",
    "relevant_repos = []\n",
    "count = 0\n",
    "for repo in repo_links:\n",
    "    commits = []\n",
    "\n",
    "    if count % 100 == 0:\n",
    "        print(\"Got to {}\".format(count))\n",
    "\n",
    "    try:\n",
    "        # For each commit in the repository\n",
    "        for commit in Repository(repo).traverse_commits():\n",
    "            # If any of the keyword appear in the commit message\n",
    "            if commit.msg is not None and any(key in commit.msg.lower() for key in cost_keywords):\n",
    "                changed_files = []\n",
    "                # Save the modified files\n",
    "                for file in commit.modified_files:\n",
    "                    changed_files.append(file.filename)\n",
    "                commit_dic = {\"id\": commit.hash, \n",
    "                              \"msg\":commit.msg, \n",
    "                              \"date\":str(commit.author_date),\n",
    "                              \"modified_files\": changed_files}\n",
    "                commits.append(commit_dic)\n",
    "        repo_dic = {\"name\":repo, \"commits\":commits}\n",
    "\n",
    "        # Mark the repository as relevant if it has any relevant commits\n",
    "        if len(commits) != 0:\n",
    "            relevant_repos.append(repo_dic)\n",
    "    except Exception as e:\n",
    "        # so that we document what errors can happen when accessing commits\n",
    "        print(f\"{e}\\n{repo}\")\n",
    "        with open(STEP3_ERRORREPOS, \"a\") as file:\n",
    "            file.write(f\"{repo}\\n\")\n",
    "    count = count + 1\n",
    "\n",
    "output = {\"no_of_repos\":len(relevant_repos) ,\"repositories\": relevant_repos}\n",
    "with open(STEP3_KWCOMMITS, \"w\") as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 4 - Filter commits that modify Terraform files \n",
    "\n",
    "Refines the previous JSON file so that only commits that modify '`.tf`' and '`.tf.json`' files are taken into consideration.\n",
    "\n",
    "The final list of filtered commits is saved as JSON in '`data/step4-tf-commits.json`'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "terraform_output = open(STEP3_KWCOMMITS)\n",
    "selected_repos = json.load(terraform_output)\n",
    "\n",
    "filtered_repos = []\n",
    "terraform_keywords = ['.tf', '.tf.json']\n",
    "print(len(selected_repos[\"repositories\"]))\n",
    "\n",
    "for repo in selected_repos[\"repositories\"]:\n",
    "    relevant_commits = []\n",
    "    flag = False\n",
    "    for commit in repo[\"commits\"]:\n",
    "        for mod_file in commit[\"modified_files\"]:\n",
    "            if mod_file is not None and any(key in mod_file for key in terraform_keywords):\n",
    "                relevant_commits.append(commit)\n",
    "                flag = True\n",
    "                break\n",
    "\n",
    "    if flag:\n",
    "        # new_commit_repo = {\"name\":repo[\"name\"], \"commits\":relevant_commits}\n",
    "        repo[\"commits\"] = relevant_commits\n",
    "        filtered_repos.append(repo)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Identified {len(filtered_repos)}\")\n",
    "\n",
    "output = {\"no_of_repos\":len(filtered_repos) ,\"repositories\": filtered_repos}\n",
    "with open(STEP4_TFCOMMITS, \"w\") as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
